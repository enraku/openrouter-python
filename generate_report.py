#!/usr/bin/env python3
"""Generate comprehensive markdown report from analysis results."""

import json
import os
from datetime import datetime
from typing import Dict, List, Optional


def load_analysis_data() -> Dict:
    """Load analysis data from available sources."""
    # Try simple test results first
    if os.path.exists("simple_model_test_results.json"):
        with open("simple_model_test_results.json", "r", encoding="utf-8") as f:
            return json.load(f)
    
    # Fallback to comprehensive analysis
    elif os.path.exists("model_analysis_report.json"):
        with open("model_analysis_report.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            # Convert to simple format for consistency
            simple_results = []
            for model in data.get("models", []):
                if model["availability"]["available"]:
                    simple_results.append({
                        "id": model["id"],
                        "available": True,
                        "response_time": model["performance"]["avg_response_time"],
                        "response_quality": 100,  # Placeholder
                        "japanese_capable": model["quality"]["japanese_score"] and model["quality"]["japanese_score"] > 0.3,
                        "error": None
                    })
            
            return {
                "timestamp": data["metadata"]["timestamp"],
                "tested_models": len(simple_results),
                "available_models": len(simple_results),
                "japanese_capable": len([r for r in simple_results if r["japanese_capable"]]),
                "results": simple_results
            }
    
    else:
        return {}


def format_time(seconds: Optional[float]) -> str:
    """Format response time."""
    if seconds is None:
        return "N/A"
    return f"{seconds:.2f}s"


def format_status(available: bool, error: Optional[str]) -> str:
    """Format availability status."""
    if available:
        return "âœ… åˆ©ç”¨å¯èƒ½"
    elif error and "Rate limit" in error:
        return "â¸ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™"
    else:
        return "âŒ ã‚¨ãƒ©ãƒ¼"


def format_japanese(capable: bool) -> str:
    """Format Japanese capability."""
    return "ğŸ‡¯ğŸ‡µ å¯¾å¿œ" if capable else "âŒ éå¯¾å¿œ"


def generate_performance_table(results: List[Dict]) -> str:
    """Generate performance comparison table."""
    # Filter and sort available models by response time
    available_models = [r for r in results if r["available"] and r["response_time"]]
    available_models.sort(key=lambda x: x["response_time"])
    
    table = "| é †ä½ | ãƒ¢ãƒ‡ãƒ«å | å¿œç­”æ™‚é–“ | æ—¥æœ¬èªå¯¾å¿œ | ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ |\n"
    table += "|------|----------|----------|------------|------------|\n"
    
    for i, model in enumerate(available_models, 1):
        model_name = model["id"]
        response_time = format_time(model["response_time"])
        japanese = format_japanese(model["japanese_capable"])
        status = format_status(model["available"], model.get("error"))
        
        table += f"| {i} | `{model_name}` | {response_time} | {japanese} | {status} |\n"
    
    # Add unavailable models
    unavailable_models = [r for r in results if not r["available"]]
    for model in unavailable_models:
        model_name = model["id"]
        status = format_status(model["available"], model.get("error"))
        table += f"| - | `{model_name}` | - | - | {status} |\n"
    
    return table


def generate_detailed_comparison_table(results: List[Dict]) -> str:
    """Generate detailed comparison table."""
    available_models = [r for r in results if r["available"]]
    available_models.sort(key=lambda x: x["response_time"] or 999)
    
    table = "| ãƒ¢ãƒ‡ãƒ«å | å¿œç­”æ™‚é–“ | ãƒ¬ã‚¹ãƒãƒ³ã‚¹å“è³ª | æ—¥æœ¬èªå¯¾å¿œ | ã‚³ã‚¹ãƒˆ | æ¨å¥¨ç”¨é€” |\n"
    table += "|----------|----------|----------------|------------|--------|----------|\n"
    
    for model in available_models:
        model_name = model["id"].replace(":", ":<br>").replace("/", "/<br>")  # æ”¹è¡Œã§è¦‹ã‚„ã™ã
        response_time = format_time(model["response_time"])
        quality = f"{model.get('response_quality', 0)}æ–‡å­—" if model.get('response_quality') else "N/A"
        japanese = "âœ…" if model["japanese_capable"] else "âŒ"
        cost = "ğŸ†“ ç„¡æ–™"
        
        # æ¨å¥¨ç”¨é€”ã‚’æ±ºå®š
        if model["japanese_capable"] and model["response_time"] and model["response_time"] < 2.5:
            use_case = "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¿ã‚¹ã‚¯<br>âš¡ é«˜é€Ÿå‡¦ç†"
        elif model["japanese_capable"]:
            use_case = "ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¿ã‚¹ã‚¯"
        elif model["response_time"] and model["response_time"] < 2.5:
            use_case = "âš¡ é«˜é€Ÿå‡¦ç†<br>ğŸ”° åˆå¿ƒè€…å‘ã‘"
        else:
            use_case = "ğŸ”° åˆå¿ƒè€…å‘ã‘<br>ğŸ“ ãƒ†ã‚¹ãƒˆç”¨"
        
        table += f"| `{model_name}` | {response_time} | {quality} | {japanese} | {cost} | {use_case} |\n"
    
    return table


def generate_summary_stats(data: Dict) -> Dict:
    """Generate summary statistics."""
    results = data.get("results", [])
    available_models = [r for r in results if r["available"]]
    
    if not available_models:
        return {
            "total_tested": len(results),
            "available_count": 0,
            "japanese_count": 0,
            "avg_response_time": 0,
            "fastest_model": None,
            "best_japanese": None
        }
    
    response_times = [r["response_time"] for r in available_models if r["response_time"]]
    japanese_models = [r for r in available_models if r["japanese_capable"]]
    
    fastest = min(available_models, key=lambda x: x["response_time"] or 999) if available_models else None
    best_japanese = japanese_models[0] if japanese_models else None
    
    return {
        "total_tested": len(results),
        "available_count": len(available_models),
        "japanese_count": len(japanese_models),
        "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
        "fastest_model": fastest,
        "best_japanese": best_japanese
    }


def generate_report(output_file: str = "openrouter_analysis_report.md"):
    """Generate comprehensive markdown report."""
    print("ğŸ¦ˆ Generating OpenRouter analysis report...")
    
    data = load_analysis_data()
    if not data:
        print("âŒ No analysis data found!")
        return
    
    stats = generate_summary_stats(data)
    results = data.get("results", [])
    
    # Generate report content
    report = f"""# OpenRouter ãƒ¢ãƒ‡ãƒ«åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ğŸ¦ˆ

**åˆ†ææ—¥æ™‚**: {data.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}  
**åˆ†æè€…**: gura ğŸ¦ˆ  
**OpenRouter Python Client**: v0.1.0

---

## ğŸ“Š ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ä¸»è¦æŒ‡æ¨™
- **ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«**: {stats['total_tested']}å€‹
- **åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«**: {stats['available_count']}å€‹ ({stats['available_count']/stats['total_tested']*100:.1f}%)
- **æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«**: {stats['japanese_count']}å€‹ ({stats['japanese_count']/stats['total_tested']*100:.1f}%)
- **å¹³å‡å¿œç­”æ™‚é–“**: {stats['avg_response_time']:.2f}ç§’

### ğŸ† æ¨å¥¨ãƒ¢ãƒ‡ãƒ«
"""
    
    if stats['fastest_model']:
        report += f"- **âš¡ æœ€é«˜é€Ÿãƒ¢ãƒ‡ãƒ«**: `{stats['fastest_model']['id']}` ({format_time(stats['fastest_model']['response_time'])})\n"
    
    if stats['best_japanese']:
        report += f"- **ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªæœ€å„ªç§€**: `{stats['best_japanese']['id']}` ({format_time(stats['best_japanese']['response_time'])})\n"
    
    if stats['available_count'] > 0:
        best_overall = min([r for r in results if r["available"]], 
                          key=lambda x: (not x["japanese_capable"], x["response_time"] or 999))
        report += f"- **ğŸŒŸ ç·åˆæ¨å¥¨**: `{best_overall['id']}`\n"
    
    report += f"""
---

## ğŸ¯ ãƒ†ã‚¹ãƒˆæ¦‚è¦

### ãƒ†ã‚¹ãƒˆå¯¾è±¡
- **ç„¡æ–™ãƒ¢ãƒ‡ãƒ«**: {stats['total_tested']}ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
- **è©•ä¾¡é …ç›®**: å¿œç­”é€Ÿåº¦ã€æ—¥æœ¬èªèƒ½åŠ›ã€å¯ç”¨æ€§

### ãƒ†ã‚¹ãƒˆæ–¹æ³•
1. **æ€§èƒ½ãƒ†ã‚¹ãƒˆ**: å˜ä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¿œç­”æ™‚é–“æ¸¬å®š
2. **æ—¥æœ¬èªãƒ†ã‚¹ãƒˆ**: ã€ŒHello! Please respond in both English and Japanese (æ—¥æœ¬èª).ã€ã§ã®æ—¥æœ¬èªæ–‡å­—æ¤œå‡º
3. **å¯ç”¨æ€§ãƒ†ã‚¹ãƒˆ**: APIå¿œç­”æˆåŠŸ/å¤±æ•—ã®ç¢ºèª

### è©•ä¾¡åŸºæº–
- **å¿œç­”æ™‚é–“**: 1ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‡¦ç†æ™‚é–“ï¼ˆç§’ï¼‰
- **æ—¥æœ¬èªèƒ½åŠ›**: ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠãƒ»æ¼¢å­—ã®ä½¿ç”¨æœ‰ç„¡
- **å¯ç”¨æ€§**: APIæ­£å¸¸å¿œç­”ã®å¯å¦

---

## ğŸ“ˆ è©³ç´°åˆ†æçµæœ

### 1. æ€§èƒ½ãƒ©ãƒ³ã‚­ãƒ³ã‚°

{generate_performance_table(results)}

### 2. è©³ç´°æ¯”è¼ƒè¡¨

{generate_detailed_comparison_table(results)}

### 3. çµ±è¨ˆã‚µãƒãƒªãƒ¼

| é …ç›® | å€¤ |
|------|-----|
| ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ—¥æ™‚ | {data.get('timestamp', 'N/A')} |
| ç·ãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«æ•° | {stats['total_tested']} |
| åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«æ•° | {stats['available_count']} |
| æˆåŠŸç‡ | {stats['available_count']/stats['total_tested']*100:.1f}% |
| æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«æ•° | {stats['japanese_count']} |
| æ—¥æœ¬èªå¯¾å¿œç‡ | {stats['japanese_count']/stats['total_tested']*100:.1f}% |
| å¹³å‡å¿œç­”æ™‚é–“ | {stats['avg_response_time']:.2f}ç§’ |
"""

    if stats['available_count'] > 0:
        available_models = [r for r in results if r["available"] and r["response_time"]]
        if available_models:
            fastest_time = min(r["response_time"] for r in available_models)
            slowest_time = max(r["response_time"] for r in available_models)
            report += f"| æœ€é€Ÿå¿œç­”æ™‚é–“ | {fastest_time:.2f}ç§’ |\n"
            report += f"| æœ€é…å¿œç­”æ™‚é–“ | {slowest_time:.2f}ç§’ |\n"

    report += f"""
---

## ğŸ’¡ ç”¨é€”åˆ¥æ¨å¥¨

### ğŸ”° åˆå¿ƒè€…ãƒ»å­¦ç¿’ç”¨
"""
    
    if stats['available_count'] > 0:
        # å®‰å®šæ€§é‡è¦–ï¼ˆã‚¨ãƒ©ãƒ¼ãªã—ã€é©åº¦ãªé€Ÿåº¦ï¼‰
        stable_models = [r for r in results if r["available"] and r["response_time"]]
        if stable_models:
            beginner_model = min(stable_models, key=lambda x: x["response_time"])
            report += f"""**æ¨å¥¨**: `{beginner_model['id']}`
- **å¿œç­”æ™‚é–“**: {format_time(beginner_model['response_time'])}
- **ã‚³ã‚¹ãƒˆ**: ğŸ†“ å®Œå…¨ç„¡æ–™
- **ç†ç”±**: å®‰å®šã—ãŸå¿œç­”ã€é©åº¦ãªé€Ÿåº¦
- **æ³¨æ„**: {'æ—¥æœ¬èªå¯¾å¿œã‚ã‚Š' if beginner_model['japanese_capable'] else 'æ—¥æœ¬èªå¯¾å¿œãªã—'}

"""
    
    if stats['japanese_count'] > 0:
        report += f"""### ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªã‚¿ã‚¹ã‚¯
**æ¨å¥¨**: `{stats['best_japanese']['id']}`
- **å¿œç­”æ™‚é–“**: {format_time(stats['best_japanese']['response_time'])}
- **æ—¥æœ¬èªèƒ½åŠ›**: âœ… å¯¾å¿œ
- **ã‚³ã‚¹ãƒˆ**: ğŸ†“ å®Œå…¨ç„¡æ–™
- **ç†ç”±**: ãƒ†ã‚¹ãƒˆå¯¾è±¡ä¸­å”¯ä¸€ã®æ—¥æœ¬èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«

"""
    
    if stats['fastest_model']:
        report += f"""### âš¡ é«˜é€Ÿå‡¦ç†
**æ¨å¥¨**: `{stats['fastest_model']['id']}`
- **å¿œç­”æ™‚é–“**: {format_time(stats['fastest_model']['response_time'])} (æœ€é€Ÿ)
- **å®‰å®šæ€§**: âœ… é«˜ã„
- **ã‚³ã‚¹ãƒˆ**: ğŸ†“ ç„¡æ–™
- **ç†ç”±**: ãƒ†ã‚¹ãƒˆå¯¾è±¡ä¸­æœ€é«˜é€Ÿåº¦

"""
    
    report += f"""### ğŸ’° æœ¬ç•ªé‹ç”¨
**æ¨å¥¨**: æœ‰æ–™ãƒ¢ãƒ‡ãƒ«æ¤œè¨
- **ç†ç”±**: ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãªã—ã€é«˜å“è³ªã€å®‰å®šæ€§
- **é¸æŠè‚¢**: Claudeã€GPTã€Geminiã‚·ãƒªãƒ¼ã‚º
- **æ³¨æ„**: è©³ç´°åˆ†æãƒ»ã‚³ã‚¹ãƒˆæ¤œè¨ãŒå¿…è¦

---

## âš ï¸ åˆ¶é™äº‹é …ãƒ»æ³¨æ„ç‚¹

### ãƒ†ã‚¹ãƒˆåˆ¶é™
- **ã‚µãƒ³ãƒ—ãƒ«æ•°**: ç„¡æ–™ãƒ¢ãƒ‡ãƒ«{stats['total_tested']}å€‹ã®ã¿
- **ãƒ†ã‚¹ãƒˆå›æ•°**: å„ãƒ¢ãƒ‡ãƒ«1å›ã®å˜ç™ºãƒ†ã‚¹ãƒˆ
- **è©•ä¾¡æœŸé–“**: çŸ­æ™‚é–“ã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ¸¬å®š

### æŠ€è¡“çš„åˆ¶é™
- **ãƒ¬ãƒ¼ãƒˆåˆ¶é™**: ä¸€éƒ¨ç„¡æ–™ãƒ¢ãƒ‡ãƒ«ã§ç™ºç”Ÿ
- **å¯ç”¨æ€§å¤‰å‹•**: æ™‚é–“å¸¯ãƒ»åœ°åŸŸã«ã‚ˆã‚‹å·®ç•°ã®å¯èƒ½æ€§
- **ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å½±éŸ¿**: é€šä¿¡ç’°å¢ƒã«ã‚ˆã‚‹æ¸¬å®šèª¤å·®

### æ—¥æœ¬èªè©•ä¾¡ã®é™ç•Œ
- **ç°¡æ˜“è©•ä¾¡**: æ–‡å­—ç¨®ã®å­˜åœ¨ã®ã¿ã§åˆ¤å®š
- **æ–‡è„ˆç†è§£**: æ–‡æ³•ãƒ»æ„å‘³ç†è§£ã¯æœªè©•ä¾¡
- **å°‚é–€æ€§**: æŠ€è¡“ç”¨èªãƒ»æ•¬èªç­‰ã¯æœªæ¤œè¨¼

---

## ğŸš€ ä»Šå¾Œã®æ”¹å–„æ¡ˆ

### åˆ†ææ‹¡å¼µ
1. **æœ‰æ–™ãƒ¢ãƒ‡ãƒ«åˆ†æ**: ä¸»è¦æœ‰æ–™ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡
2. **ç¶™ç¶šç›£è¦–**: 24æ™‚é–“å¯ç”¨æ€§ãƒ»ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¿½è·¡
3. **å“è³ªè©•ä¾¡**: BLEU/ROUGEã‚¹ã‚³ã‚¢ã€äººé–“è©•ä¾¡å°å…¥

### æ©Ÿèƒ½è¿½åŠ 
1. **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–**: ãƒ¢ãƒ‡ãƒ«å¯ç”¨æ€§ã®ç¶™ç¶šç›£è¦–
2. **ã‚³ã‚¹ãƒˆè¨ˆç®—æ©Ÿ**: ä½¿ç”¨é‡ãƒ™ãƒ¼ã‚¹ã‚³ã‚¹ãƒˆäºˆæ¸¬ãƒ„ãƒ¼ãƒ«
3. **ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**: æ¨™æº–ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½æ¯”è¼ƒ

### UI/UXæ”¹å–„
1. **Webãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**: ãƒ–ãƒ©ã‚¦ã‚¶ãƒ™ãƒ¼ã‚¹åˆ†æã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
2. **ã‚°ãƒ©ãƒ•åŒ–**: æ€§èƒ½ãƒ»ã‚³ã‚¹ãƒˆã®å¯è¦–åŒ–
3. **ã‚¢ãƒ©ãƒ¼ãƒˆæ©Ÿèƒ½**: æ¨å¥¨ãƒ¢ãƒ‡ãƒ«å¤‰æ›´ã®è‡ªå‹•é€šçŸ¥

---

## ğŸ“š æŠ€è¡“ä»•æ§˜

### ä½¿ç”¨æŠ€è¡“
- **OpenRouter Python Client**: v0.1.0 (ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…)
- **è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: ç‹¬è‡ªé–‹ç™ºPython ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- **ãƒ‡ãƒ¼ã‚¿å½¢å¼**: JSON (åˆ†æçµæœ) + Markdown (ãƒ¬ãƒãƒ¼ãƒˆ)

### å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
```bash
# ç°¡æ˜“ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python simple_model_test.py

# åŒ…æ‹¬çš„åˆ†æå®Ÿè¡Œ  
python model_analyzer.py

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
python generate_report.py
```

### ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ
```
â”œâ”€â”€ simple_model_test.py          # è»½é‡ãƒ†ã‚¹ãƒˆãƒ„ãƒ¼ãƒ«
â”œâ”€â”€ model_analyzer.py             # åŒ…æ‹¬çš„åˆ†æãƒ„ãƒ¼ãƒ«
â”œâ”€â”€ generate_report.py            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
â”œâ”€â”€ model_summary.py              # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
â”œâ”€â”€ simple_model_test_results.json # ãƒ†ã‚¹ãƒˆç”Ÿãƒ‡ãƒ¼ã‚¿
â””â”€â”€ openrouter_analysis_report.md  # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
```

---

## ğŸ“ ãŠå•ã„åˆã‚ã›ãƒ»è²¢çŒ®

**é–‹ç™ºè€…**: gura ğŸ¦ˆ  
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: OpenRouter Python Client  
**GitHub**: https://github.com/enraku/openrouter-python  
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: MIT

### è²¢çŒ®æ–¹æ³•
1. **Issues**: ãƒã‚°å ±å‘Šãƒ»æ©Ÿèƒ½è¦æœ›
2. **Pull Requests**: ã‚³ãƒ¼ãƒ‰æ”¹å–„ãƒ»æ–°æ©Ÿèƒ½
3. **ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**: åˆ†æçµæœã®æ¤œè¨¼ãƒ»æ”¹å–„ææ¡ˆ

---

*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚æœ€æ–°æƒ…å ±ã«ã¤ã„ã¦ã¯å†åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚*

**ç”Ÿæˆæ—¥æ™‚**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹**: {len(results)}ãƒ¢ãƒ‡ãƒ«ã®å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿
"""
    
    # Save report
    with open(output_file, "w", encoding="utf-8") as f:
        f.write(report)
    
    print(f"âœ… Report generated: {output_file}")
    print(f"ğŸ“Š Analyzed {stats['total_tested']} models")
    print(f"ğŸ“ˆ {stats['available_count']} models available")
    print(f"ğŸ‡¯ğŸ‡µ {stats['japanese_count']} models support Japanese")


if __name__ == "__main__":
    generate_report()